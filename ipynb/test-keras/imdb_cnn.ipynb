{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 201s - loss: 0.4161 - acc: 0.7990 - val_loss: 0.2939 - val_acc: 0.8780\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 187s - loss: 0.2494 - acc: 0.8983 - val_loss: 0.2834 - val_acc: 0.8814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12de56a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [1, 89, 27, 2, 2, 17, 199, 132, 5, 4191, 16, 1339, 24, 8, 760, 4, 1385, 7, 4, 22, 1368, 2, 16, 2, 17, 1635, 7, 2, 1368, 9, 4, 1357, 8, 14, 991, 13, 877, 38, 19, 27, 239, 13, 100, 235, 61, 483, 2, 4, 7, 4, 20, 131, 1102, 72, 8, 14, 251, 27, 1146, 7, 308, 16, 735, 1517, 17, 29, 144, 28, 77, 2305, 18, 12],\n",
       "       [1, 3452, 7, 2, 517, 522, 31, 314, 17, 1909, 2046, 2, 2, 2, 83, 4, 2314, 673, 33, 27, 568, 1709, 2923, 32, 4, 189, 22, 11, 975, 4135, 29, 2376, 4, 1287, 7, 4, 2, 4217, 15, 1435, 455, 1394, 848, 1538, 4031, 96, 145, 11, 4, 204, 2, 297, 2, 29, 3044, 4, 1287, 8, 35, 4383, 1609, 121, 2, 1233, 980, 2, 2100, 2, 2, 2, 3681, 304, 4, 1287, 145, 8, 41, 1472, 50, 2, 2, 2, 2, 4364, 34, 2782, 2, 145, 295, 174, 772, 6, 2, 18, 274, 961, 90, 145, 8, 4041, 113, 155, 92, 140, 17, 2, 69, 3205, 2, 505, 46, 24, 8, 30, 4, 132, 7, 41, 1306, 103, 32, 38, 59, 2, 90, 11, 6, 297, 2, 33, 63, 2, 9, 329, 74, 654, 137, 2, 304, 6, 4548, 2, 2949, 2, 41, 772, 15, 274, 961, 41, 145, 8, 113, 11, 4, 2995, 7, 6, 668, 4217, 1810, 17, 6, 3452, 1082, 181, 8, 30, 1571, 11, 3161, 2350, 28, 8, 157, 295, 8, 79, 8, 6, 2, 11, 162, 2, 121, 2, 1249, 648, 69, 77, 3554, 19, 4, 2, 887, 8, 4416, 68, 4123, 145, 83, 406, 2350, 4, 2350, 7, 2, 2, 3509, 1851, 27, 980, 2, 2, 2, 37, 26, 199, 23, 4, 521, 39, 3408, 1697, 2297, 7, 568, 3864, 2, 308, 3659, 80, 81, 1780, 10, 10, 526, 34, 2, 2, 13, 119, 3452, 7, 2, 4, 229, 34, 1561, 2, 9, 87, 253, 55, 702, 728, 545, 441, 2072, 958, 7, 85, 189, 22, 19, 52, 2, 39, 4, 636, 720, 121, 75, 67, 1655, 2, 2, 2377, 39, 4, 2553, 4, 4971, 108, 2281, 2, 2, 4626, 2, 39, 4, 6, 1726, 23, 4903, 890, 201, 488, 4664, 2377, 39, 4, 2195, 3135, 8, 4, 2974, 343, 39, 3452, 7, 2, 2, 54, 12, 2360, 2, 4, 172, 136, 3452, 7, 2, 115, 304, 410, 615, 63, 9, 43, 17, 73, 50, 26, 775, 7, 31, 2433, 532, 2, 1994, 15, 2039, 4142, 93, 2, 6, 171, 153, 908, 12, 152, 306, 1595, 8, 2, 253, 33, 410, 4, 189, 512, 11, 831, 13, 119, 4, 136, 54, 3509, 2, 26, 260, 6, 2711, 2, 731, 2599, 15, 2, 2, 29, 166, 163, 2, 795, 2, 469, 198, 24, 8, 135, 15, 50, 218, 6, 1543, 52, 22, 11, 50, 17, 73, 88, 50, 91, 434, 9, 167, 2, 1030, 8, 987, 52, 841, 6, 147, 281, 7, 253, 199, 406, 3161, 732, 7, 105, 26, 1451, 4091, 17, 257, 2162, 2712, 68, 205, 732, 7, 4816, 712, 15, 4, 4951, 7, 2, 15, 36, 26, 1200, 496, 62, 540, 1203, 2536, 3452, 7, 2, 9, 87, 18, 4, 91, 173, 47, 15, 194, 352, 2, 44, 12, 33, 44, 2476, 1782, 1782, 13, 144, 440, 38, 4, 64, 155, 15, 13, 80, 135, 9, 15, 49, 7, 4, 2, 302, 34, 1842, 26, 6, 117, 3463, 2631, 13, 191, 377, 101, 1683, 139, 11, 3452, 7, 2, 345, 2670, 4, 22, 152, 2, 4, 541, 599, 19, 6, 646, 2, 3681, 2, 2, 83, 4472, 393, 11, 3532, 6, 2, 2, 3490, 84, 2, 23, 2, 7, 3062, 294, 112, 2, 34, 6, 666, 2832, 6, 3314, 125, 2, 2, 998, 2, 2, 4, 116, 9, 184, 52, 2, 17, 2, 9, 55, 163, 17, 29, 2, 4, 31, 2433, 46, 13, 82, 40, 4, 139, 19, 2, 33, 4, 454, 169, 41, 55, 1279, 54, 442, 1658, 32, 15, 2, 2, 13, 191, 30, 4, 64, 31, 1348, 13, 1276, 104, 3452, 7, 2, 9, 6, 777, 22, 964, 722, 39, 380, 8, 1363, 87, 1285, 189, 11, 3215, 4160, 33, 64, 2, 234, 196, 12, 115, 461, 357, 42, 753, 6, 965, 1640, 7, 1923, 106, 12, 17, 515, 17, 25, 70],\n",
       "       [1, 1868, 256, 34, 31, 7, 4, 91, 2305, 1507, 7, 4, 236, 2068, 7, 14, 1117, 5, 82, 31, 7, 4, 91, 1020, 1507, 2, 4686, 46, 7, 2415, 59, 9, 389, 9, 175, 173, 15, 59, 299, 4, 2, 2, 9, 4, 3114, 5, 1805, 7, 4, 298, 438, 10, 10, 2, 3365, 9, 2, 5, 41, 658, 742, 217, 73, 1391, 34, 530, 284, 5, 82, 735, 2286, 1024, 1487, 3740, 2828, 7, 4, 2, 255, 47, 6, 254, 58, 19, 4, 2, 3365, 7, 27, 31, 283, 155, 5, 4846, 27, 2, 339, 4, 338, 577, 3996, 2, 2, 1516, 2, 47, 96, 99, 76, 873, 7, 41, 57, 2010, 4, 65, 304, 6, 55, 821, 650, 23, 4, 4696, 7, 6, 4069, 11, 14, 20, 4, 64, 577, 47, 8, 276, 41, 113, 23, 1070, 8, 459, 18, 4, 738, 7, 409, 50, 9, 210, 31, 11, 175, 223, 37, 1590, 15, 243, 7, 4756, 3996, 9, 1612, 4, 454, 7, 4, 20, 21, 17, 58, 4097, 59, 630, 56, 1897, 41, 2, 113, 58, 2, 8, 41, 223, 59, 60, 1643, 41, 1633, 89, 81, 25, 81, 27, 175, 251, 11, 5, 46, 5, 1337, 2, 12, 15, 9, 51, 372, 81, 6, 176, 7, 51, 13, 683, 2504, 157, 2, 75, 2170, 75, 4290, 75, 2, 75, 3218, 75, 2, 75, 26, 4, 118, 369, 75, 26, 4, 4727, 2728, 49, 7, 178, 40, 199, 372, 11, 14, 20, 28, 4, 404, 4421, 26, 4, 1987, 2, 18, 4, 436, 223, 5, 82, 81, 32, 15, 2504, 157, 15, 9, 1868, 3996, 5, 111, 372, 11, 263, 926, 111, 7, 178, 28, 460, 825, 143, 15, 868, 7, 113, 54, 263, 846, 559, 5, 1131, 13, 28, 77, 50, 36, 43, 435, 99, 185, 13, 28, 348, 61, 846, 61, 1216, 21, 13, 115, 2717, 98, 17, 73, 17, 54, 13, 69, 8, 297, 68, 555, 5, 69, 8, 1135, 11, 68, 3730, 14, 20, 2, 4, 635, 7, 113, 382, 12, 9, 619, 21, 15, 9, 89, 113, 9, 33, 211, 742, 6, 2489, 33, 2, 9, 2732, 415, 37, 739, 8, 104, 15, 27, 157, 9, 53, 674, 74, 1462, 334, 5, 47, 6, 55, 1300, 2, 2, 1841, 4, 372, 11, 27, 113, 29, 9, 24, 565, 195, 8, 2, 48, 25, 181, 8, 67, 52, 116, 5, 4, 635, 7, 113, 81, 24, 717, 14, 20, 514, 139, 4, 3756, 582, 8, 1868, 2, 5, 32, 4, 231, 7, 6, 2702, 46, 7, 1912, 2714, 15, 13, 38, 2, 75, 26, 32, 1912, 2, 514, 4414, 742, 12, 9, 64, 34, 170, 2, 15, 25, 923, 15, 25, 26, 66, 170, 4451, 742, 25, 28, 6, 2, 4421, 21, 121, 9, 129, 483, 10, 10],\n",
       "       ...,\n",
       "       [1, 14, 390, 7, 2, 1194, 285, 4, 123, 9, 44, 8, 130, 45, 840, 811, 5, 32, 609, 9, 2244, 1888, 11, 14, 390, 4, 2, 663, 721, 35, 1356, 773, 884, 2, 8, 4, 2, 4, 2910, 90, 39, 4, 2, 2, 54, 3034, 29, 2, 11, 17, 6, 2, 5, 95, 83, 27, 2734, 2391, 29, 2, 3913, 6, 1513, 63, 484, 2, 41, 46, 5, 2201, 1098, 41, 95, 2, 2, 3913, 51, 9, 317, 7, 4, 1513, 2, 266, 39, 4, 2, 5, 560, 4, 2, 3341, 159, 385, 516, 4, 1042, 21, 112, 4, 671, 7, 31, 12, 43, 2, 90, 4892, 266, 8, 2, 4, 85, 2481, 5, 494, 8, 169, 5, 2330, 90, 18, 147, 2, 2086, 9, 11, 4, 2, 269, 8, 169, 3636, 54, 5, 2, 140, 46, 83, 4, 890, 8, 169, 4, 2734, 4, 2734, 659, 98, 103, 68, 985, 4, 4701, 923, 15, 6, 370, 1059, 285, 54, 36, 79, 145, 8, 2, 269, 8, 985, 4, 1776, 5, 103, 36, 2, 6, 6, 1718, 825, 2, 3234, 2, 1077, 41, 2, 8, 847, 84, 46, 7, 4, 96, 38, 59, 70, 79, 8, 4, 1550, 21, 36, 79, 68, 8, 522, 5, 2, 1442, 5, 43, 54, 9, 44, 8, 79, 324, 58, 9, 2, 8, 121, 36, 721, 884, 2, 8, 4, 2, 3682, 11, 5, 2, 5, 2, 21, 2, 9, 131, 11, 4, 2, 38, 1098, 4, 1042, 5, 2, 46, 7, 4, 2, 54, 4892, 417, 266, 29, 191, 2, 9, 351, 5, 38, 9, 4, 671, 7, 289, 18, 150, 1276, 14, 390, 16, 619, 16, 4, 7, 32, 7, 98, 13, 62, 119, 8, 28, 41, 671, 7, 2, 13, 66, 92, 104, 2, 144, 7, 435, 8, 4, 2, 88, 48, 59, 161, 586, 28, 556, 21, 2, 961, 4, 671, 7, 289, 295, 174, 5, 146, 654, 19, 4, 3769, 3724],\n",
       "       [1, 13, 435, 83, 14, 22, 1017, 1383, 18, 6, 2928, 1278, 11, 405, 2, 7, 4039, 2228, 21, 51, 13, 188, 16, 53, 7, 6, 1162, 3905, 1010, 19, 230, 99, 76, 662, 5, 24, 195, 206, 45, 788, 15, 14, 22, 16, 93, 23, 6, 352, 4, 1979, 26, 2, 5, 862, 324, 137, 4, 116, 889, 6, 176, 8, 30, 4630, 82, 4, 114, 2679, 23, 6, 3993, 7, 2, 6, 336, 5, 107, 3197, 15, 2114, 6, 3817, 7, 1818, 103, 880, 49, 2, 36, 216, 638, 6, 2816, 2, 34, 6, 185, 250, 5, 41, 2, 5, 32, 14, 9, 579, 11, 2183, 34, 4, 185, 250, 3880, 2, 11, 35, 2, 45, 788, 15, 907, 2393, 5, 1024, 2, 197, 36, 71, 231, 142, 66, 1621, 21, 466, 94, 118, 2048, 1226, 7, 609, 2527, 9, 43, 99, 357, 8, 1465, 4, 529, 4, 22, 2, 23, 18, 44, 2, 234, 5, 91, 7, 12, 3202, 7, 357, 105, 2, 125, 357, 5, 196, 2, 414, 4, 64, 52, 155, 13, 28, 8, 135, 44, 4, 22, 9, 19, 2, 8, 4, 228, 63, 9, 52, 11, 1370, 4, 277, 9, 4, 64, 85, 52, 155, 44, 4, 20, 5, 198, 64, 88, 45, 4, 236, 155, 15, 571, 13, 586, 386, 259, 2, 2, 14, 180, 50, 16, 76, 128, 1157, 93, 11, 4, 4039],\n",
       "       [1, 1252, 54, 13, 435, 8, 67, 14, 20, 33, 4, 2, 750, 11, 2, 13, 122, 24, 535, 76, 13, 435, 8, 14, 20, 64, 88, 13, 2626, 1400, 45, 6, 2, 20, 4092, 30, 52, 18, 6, 462, 95, 13, 1829, 180, 5, 296, 12, 5, 219, 138, 36, 2471, 2, 2, 3561, 8, 297, 2, 2, 29, 9, 242, 31, 7, 4, 2, 493, 23, 4, 194, 268, 76, 433, 11, 61, 652, 74, 2281, 42, 1655, 5, 47, 31, 194, 3079, 8, 85, 102, 15, 2, 72, 8, 6, 189, 20, 12, 287, 2, 2, 17, 294, 37, 9, 406, 29, 47, 6, 483, 57, 551, 89, 2509, 5, 948, 12, 9, 29, 764, 1460, 142, 15, 1655, 115, 127, 42, 739, 8, 123, 29, 764, 2, 5, 1742, 151, 174, 199, 7, 98, 2140, 63, 25, 80, 1495, 48, 25, 67, 4, 20, 32, 11, 32, 6, 275, 585, 11, 61, 652, 74, 111, 2, 5, 12, 770, 72, 11, 6, 171, 771, 17, 11, 37, 1452, 11, 4, 130]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datamining]",
   "language": "python",
   "name": "conda-env-datamining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
