{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluate a simple MLP\n",
    "on the Reuters newswire topic classification task.\n",
    "'''\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "[ [1, 4, 2, 2, 9, 697, 2, 111, 8, 25, 109, 29, 2, 11, 150, 244, 364, 33, 30, 30, 2, 333, 6, 2, 159, 9, 2, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 2, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 2, 966, 2, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 2, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 2, 855, 129, 783, 21, 4, 2, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 2, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n",
      " [1, 2, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 2, 19, 12, 11, 82, 52, 29, 283, 2, 558, 2, 265, 2, 2, 8, 2, 118, 371, 10, 2, 281, 4, 143, 2, 760, 50, 2, 225, 139, 683, 4, 48, 193, 862, 41, 967, 2, 30, 2, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 2, 13, 260, 951, 2, 8, 69, 2, 18, 82, 41, 30, 306, 2, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 2, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 2, 2, 2, 6, 2, 2, 4, 2, 41, 263, 84, 395, 649, 18, 82, 838, 2, 4, 572, 4, 106, 13, 25, 595, 2, 40, 85, 2, 518, 5, 4, 2, 51, 115, 680, 16, 6, 719, 250, 27, 429, 2, 8, 2, 114, 343, 84, 142, 20, 5, 2, 2, 4, 65, 494, 474, 27, 69, 445, 11, 2, 2, 8, 109, 181, 2, 2, 62, 2, 6, 624, 901, 2, 107, 4, 2, 34, 524, 4, 2, 2, 41, 447, 7, 2, 13, 69, 251, 18, 872, 876, 2, 468, 2, 242, 5, 646, 27, 2, 169, 283, 87, 9, 10, 2, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2, 6, 195, 2, 2, 268, 609, 4, 195, 41, 2, 2, 2, 4, 73, 706, 2, 92, 4, 91, 2, 36, 8, 51, 144, 23, 2, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2, 2, 269, 274, 87, 2, 8, 787, 283, 34, 596, 661, 2, 13, 2, 2, 90, 2, 84, 22, 2, 2, 54, 748, 2, 8, 87, 62, 2, 84, 161, 5, 2, 480, 4, 2, 416, 6, 538, 122, 115, 55, 129, 2, 2, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 2, 2, 64, 2, 22, 125, 2, 9, 2, 172, 399, 9, 2, 2, 9, 2, 122, 36, 8, 2, 172, 247, 100, 97, 2, 34, 75, 477, 541, 4, 283, 182, 4, 2, 295, 301, 2, 125, 2, 2, 8, 77, 57, 445, 283, 2, 217, 31, 380, 704, 51, 77, 2, 509, 5, 476, 9, 2, 122, 115, 853, 6, 2, 52, 10, 2, 2, 2, 5, 4, 283, 182, 36, 8, 2, 114, 30, 531, 6, 2, 9, 2, 529, 13, 2, 2, 58, 529, 7, 2, 2, 185, 2, 240, 2, 2, 949, 657, 57, 6, 2, 283, 36, 8, 2, 8, 4, 2, 34, 2, 13, 10, 2, 5, 4, 141, 283, 120, 50, 2, 7, 2, 43, 10, 181, 283, 734, 115, 55, 2, 476, 6, 2, 10, 73, 120, 50, 41, 2, 169, 87, 2, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2, 30, 2, 2, 161, 4, 294, 517, 23, 2, 25, 398, 9, 2, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 2, 28, 69, 61, 376, 514, 90, 2, 62, 2, 13, 4, 2, 696, 122, 404, 2, 22, 134, 6, 187, 514, 10, 2, 107, 4, 96, 2, 2, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 2, 27, 4, 153, 5, 52, 29, 36, 8, 2, 8, 612, 408, 10, 2, 283, 76, 27, 2, 31, 169, 951, 2, 122, 36, 8, 283, 236, 62, 641, 84, 618, 2, 22, 2, 2, 9, 274, 2, 399, 2, 51, 115, 55, 45, 2, 31, 4, 490, 558, 36, 8, 224, 2, 115, 57, 85, 2, 2, 5, 283, 6, 4, 37, 38, 7, 2, 185, 77, 2, 4, 555, 298, 77, 240, 2, 7, 327, 652, 194, 2, 2, 34, 2, 2, 2, 2, 6, 240, 260, 458, 87, 6, 134, 514, 10, 2, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 2, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2, 6, 2, 2, 434, 835, 22, 2, 2, 434, 835, 7, 48, 2, 8, 618, 225, 586, 333, 122, 572, 126, 2, 2, 62, 133, 6, 2, 233, 28, 602, 188, 5, 4, 704, 2, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 2, 2, 281, 4, 225, 760, 17, 12]\n",
      " [1, 4, 309, 2, 2, 5, 2, 403, 2, 33, 2, 2, 2, 87, 13, 536, 78, 2, 399, 7, 2, 212, 10, 634, 179, 8, 137, 2, 7, 2, 33, 30, 2, 43, 33, 2, 50, 489, 4, 403, 6, 96, 399, 7, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 8, 2, 2, 2, 791, 5, 740, 220, 707, 13, 4, 634, 634, 54, 2, 2, 4, 361, 182, 24, 511, 972, 137, 403, 2, 529, 6, 96, 2, 399, 41, 30, 2, 21, 10, 2, 2, 503, 5, 188, 6, 353, 26, 2, 21, 432, 4, 2, 23, 2, 435, 34, 737, 6, 246, 2, 274, 2, 2, 87, 13, 399, 992, 27, 274, 403, 87, 2, 85, 480, 52, 2, 403, 820, 13, 10, 139, 9, 115, 949, 609, 890, 819, 6, 812, 593, 7, 576, 7, 194, 2, 216, 2, 8, 2, 8, 634, 33, 768, 2, 593, 4, 403, 2, 185, 9, 107, 403, 87, 2, 107, 2, 410, 4, 682, 189, 161, 2, 762, 274, 2, 115, 30, 43, 389, 410, 4, 682, 107, 2, 762, 456, 36, 8, 184, 2, 95, 2, 107, 403, 87, 302, 2, 8, 129, 100, 756, 7, 2, 96, 298, 55, 370, 731, 866, 189, 115, 949, 2, 115, 949, 343, 756, 2, 9, 115, 949, 343, 756, 2, 36, 8, 17, 12]\n",
      " ...,\n",
      " [1, 2, 124, 53, 653, 26, 39, 2, 18, 14, 2, 18, 155, 177, 53, 544, 26, 39, 19, 2, 18, 14, 19, 2, 18, 280, 2, 11, 14, 2, 32, 11, 695, 2, 47, 11, 14, 2, 63, 11, 430, 2, 44, 11, 14, 61, 11, 17, 12]\n",
      " [1, 2, 2, 71, 8, 23, 166, 344, 10, 78, 13, 68, 80, 467, 606, 6, 261, 5, 146, 93, 124, 4, 166, 75, 2, 2, 2, 265, 2, 2, 2, 297, 2, 195, 9, 621, 575, 2, 2, 7, 378, 104, 421, 648, 20, 5, 4, 49, 2, 8, 2, 28, 4, 303, 163, 524, 10, 2, 6, 455, 4, 326, 685, 6, 2, 422, 71, 142, 73, 863, 62, 75, 2, 6, 4, 326, 166, 2, 34, 2, 2, 6, 4, 166, 4, 49, 8, 17, 12]\n",
      " [1, 706, 209, 658, 4, 37, 38, 309, 484, 4, 2, 6, 933, 4, 89, 709, 377, 101, 28, 4, 143, 511, 101, 5, 47, 758, 15, 90, 2, 7, 809, 6, 444, 2, 4, 911, 5, 709, 198, 2, 634, 2, 2, 2, 8, 2, 6, 674, 480, 10, 990, 309, 2, 2, 2, 2, 24, 68, 583, 242, 5, 4, 143, 709, 364, 2, 41, 30, 13, 706, 6, 837, 4, 377, 101, 6, 631, 28, 47, 758, 15, 36, 2, 107, 4, 377, 101, 62, 47, 758, 15, 634, 114, 713, 888, 2, 6, 343, 37, 38, 2, 95, 2, 269, 43, 2, 2, 6, 226, 2, 4, 377, 101, 136, 143, 2, 4, 89, 709, 377, 101, 2, 30, 478, 97, 47, 948, 15, 90, 2, 2, 2, 41, 30, 13, 706, 6, 455, 4, 465, 474, 6, 837, 634, 6, 2, 4, 709, 377, 101, 28, 47, 758, 15, 7, 463, 29, 89, 2, 97, 148, 16, 6, 47, 948, 15, 4, 48, 511, 377, 101, 23, 47, 758, 15, 161, 5, 4, 47, 12, 20, 2, 2, 386, 240, 2, 2, 24, 10, 181, 2, 7, 194, 534, 21, 709, 364, 756, 33, 30, 4, 386, 404, 36, 118, 4, 2, 24, 4, 911, 7, 2, 23, 24, 4, 37, 38, 377, 101, 2, 42, 2, 6, 127, 122, 9, 2, 2, 692, 13, 37, 38, 2, 446, 69, 4, 234, 709, 2, 2, 13, 126, 2, 5, 338, 458, 2, 8, 4, 2, 911, 23, 4, 307, 2, 36, 8, 634, 23, 325, 2, 4, 820, 9, 129, 2, 40, 836, 85, 2, 17, 12]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 1s - loss: 1.4111 - acc: 0.6829 - val_loss: 1.0884 - val_acc: 0.7608\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.7833 - acc: 0.8155 - val_loss: 0.9376 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.5431 - acc: 0.8671 - val_loss: 0.8822 - val_acc: 0.7931\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.4201 - acc: 0.8986 - val_loss: 0.8848 - val_acc: 0.8020\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.3290 - acc: 0.9150 - val_loss: 0.9169 - val_acc: 0.7875\n",
      "1152/2246 [==============>...............] - ETA: 0sTest score: 0.892657914336\n",
      "Test accuracy: 0.794300979519\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print()\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datamining]",
   "language": "python",
   "name": "conda-env-datamining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
